<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    
    <head>
        <meta http-equiv="Content-Language" content="en">
            <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
                <link rel="stylesheet" type="text/css" href="../style.css">
                    <title>Towards Efficient Data-flow Test Data Generation</title>
                    <meta name="keywords" content="data flow testing, data flow coverage, test data generation, KLEE, BLAST, CPAchecker, CBMC">
                        <meta name="author" content="Ting Su">
                            <meta name="description" content="A bibliography of papers related to data flow testing.">
                                
                                <style type="text/css">
                                    table.references td {
                                        padding-bottom: 5px;
                                        vertical-align: top;
                                    }
                                </style>
    </head>
    
    <body>
        
<!--        <table style="width: 100%">-->
<!--            <tr>-->
<!--                <td class="centered"><a href="http://tingsu.github.io/">Home</a></td>-->
<!--                <td class="centered"><a href="">Publications</a></td>-->
<!--                <td class="centered"><a href="">Curriculum Vitae</a></td>-->
<!--                <td class="centered"><a href="">Education</a></td>-->
<!--            </tr>-->
<!--        </table>-->

        <hr>
        
        <h1>Towards Efficient Data-flow Test Data Generation</h1>
        
        <h2>Overview</h2>
        
        <p><a href="http://tingsu.github.io/files/data-flow-testing-survey.pdf">Data-flow testing (DFT)</a> aims to detect potential data interaction anomalies by focusing on the points at which variables receive values and the points at which these values are used. Such test objectives are referred as def-use pairs. However, the complexity of DFT still overwhelms the testers in practice. To tackle this problem, we introduce a hybrid testing framework for data-flow based test generation: (1) The core of our framework is symbolic execution (SE), enhanced by a novel guided path exploration strategy to improve testing performance; and (2) we systematically cast DFT as reachability checking in software model checking (SMC) to complement SE, yielding practical DFT that combines the two techniques’ strengths. We implemented our framework for C programs on top of the state-of-the-art symbolic execution engine KLEE and instantiated with three different software model checkers. Our evaluation on the 28,354 def-use pairs collected from 33 open-source and industrial program subjects shows (1) our SE-based approach can improve DFT performance by 15∼48%
in terms of testing time, compared with state-of-the-art search strategies; and (2) our combined approach can further reduce testing time by 20.1∼93.6%, and improve data-flow coverage by 27.8∼45.2% by eliminating infeasible test objectives. This hybrid approach also enables the cross-checking of each component for reliable and robust testing results. We have made our testing framework and benchmarks publicly available to facilitate future research.</p>
        
	        
        <h2>Approach Workflow</h2>
        
        <td class="centered">
            <img src="https://tingsu.github.io/files/hybrid_dft_framework.png" width="500" height="350"></td>
        <td>

		<h2>Tools (<a href="https://github.com/tingsu/hybrid_dft">Download</a>)</h2>
		
			
			<p>Our approach was built on four different tools, which are summarized as follows.</p>
			<p>Our version of <a href="https://github.com/muchang/klee">KLEE</a> can be downloaded, which is adapted for data-flow testing.</p> 

			<table border="1">
				<tr>
					<th>Name</th>
					<th>Approach</th>
					<th>Technique</th>
					<th>Version</th>
				</tr>

				<tr>
				    <td><a href="https://klee.github.io/">KLEE</a></td>
				    <td>Symbolic Execution</td>
				    <td>SE</td>
				    <td>v1.1.0</td>
				</tr>

				<tr>
				    <td><a href="http://mtc.epfl.ch/software-tools/blast/index-epfl.php">BLAST</a></td>
				    <td>Model Checking</td>
				    <td>CEGAR</td>
				    <td>v2.7.3</td>
				</tr>

				<tr>
				    <td><a href="https://cpachecker.sosy-lab.org/">CPAchecker</a></td>
				    <td>Model Checking</td>
				    <td>CEGAR</td>
				    <td>v1.7</td>
				</tr>

				<tr>
				    <td><a href="http://www.cprover.org/cbmc/">CBMC</a></td>
				    <td>Model Checking</td>
				    <td>BMC</td>
				    <td>v5.7</td>
				</tr>
			</table>

			

		<h2>Benchmark (<a href="https://github.com/tingsu/hybrid_dft">Download</a>)</h2>

		<p>We carefully constructed a repository of benchmark subjects for evaluating data-flow testing techniques. It includes subjects from four sources:</p>

		<ul>
            <li><a href="https://tingsu.github.io/files/dftbib.html">Existing literature</a> on data-flow testing</li>
            <li>Siemens artifacts from <a href="http://sir.unl.edu/php/previewfiles.php">SIR</a></li>
            <li>Subjects from <a href="https://sv-comp.sosy-lab.org/2017/">SV-COMP</a></li>
			<li>Subjects from our industrial research partners (automobile, space, subway companies)</li>
        </ul>

		<p>CBMC uses --unwind and --depth, respectively, to limit the number of times loops to be unwound and the number of program steps to be processed. The parameters used in our evaluation are listed below.</p>

		<table border="1">
				<tr>
					<th>Name</th>
					<th>CBMC's --unwind</th>
					<th>CBMC's --depth</th>
				</tr>

				<tr>
				    <td>factorization</td>
				    <td>4</td>
				    <td>90</td>
				</tr>

				<tr>
				    <td>power</td>
				    <td>2</td>
				    <td>40</td>
				</tr>

				<tr>
				    <td>find</td>
				    <td>4</td>
				    <td>90</td>
				</tr>

				<tr>
				    <td>triangle</td>
				    <td>2</td>
				    <td>40</td>
				</tr>

				<tr>
				    <td>strmat</td>
				    <td>3</td>
				    <td>60</td>
				</tr>

				<tr>
				    <td>strmat2</td>
				    <td>3</td>
				    <td>70</td>
				</tr>

				<tr>
				    <td>textfmt</td>
				    <td>25</td>
				    <td>700</td>
				</tr>

				<tr>
				    <td>tcas</td>
				    <td>2</td>
				    <td>200</td>
				</tr>

				<tr>
				    <td>replace</td>
				    <td>6</td>
				    <td>350</td>
				</tr>

				<tr>
				    <td>totinfo</td>
				    <td>15</td>
				    <td>250</td>
				</tr>

				<tr>
				    <td>printtokens</td>
				    <td>30</td>
				    <td>300</td>
				</tr>

				<tr>
				    <td>printtokens2</td>
				    <td>12</td>
				    <td>450</td>
				</tr>

				<tr>
				    <td>schedule</td>
				    <td>5</td>
				    <td>350</td>
				</tr>

				<tr>
				    <td>schedule2</td>
				    <td>5</td>
				    <td>250</td>
				</tr>

				<tr>
				    <td>kbfiltr</td>
				    <td>2</td>
				    <td>250</td>
				</tr>

				<tr>
				    <td>kbfiltr2</td>
				    <td>2</td>
				    <td>300</td>
				</tr>

				<tr>
				    <td>diskperf</td>
				    <td>2</td>
				    <td>650</td>
				</tr>

				<tr>
				    <td>floppy</td>
				    <td>2</td>
				    <td>500</td>
				</tr>

				<tr>
				    <td>floppy2</td>
				    <td>2</td>
				    <td>500</td>
				</tr>

				<tr>
				    <td>cdaudio</td>
				    <td>2</td>
				    <td>500</td>
				</tr>

				<tr>
				    <td>s3_clnt</td>
				    <td>13</td>
				    <td>500</td>
				</tr>

				<tr>
				    <td>s3_clnt_termination</td>
				    <td>13</td>
				    <td>400</td>
				</tr>
		
				<tr>
				    <td>s3_srvr_1a</td>
				    <td>10</td>
				    <td>200</td>
				</tr>

				<tr>
				    <td>s3_srvr_1b</td>
				    <td>4</td>
				    <td>100</td>
				</tr>

				<tr>
				    <td>s3_srvr_2</td>
				    <td>15</td>
				    <td>1450</td>
				</tr>

				<tr>
				    <td>s3_srvr_7</td>
				    <td>15</td>
				    <td>1450</td>
				</tr>

				<tr>
				    <td>s3_srvr_8</td>
				    <td>15</td>
				    <td>1450</td>
				</tr>

				<tr>
				    <td>s3_srvr_10</td>
				    <td>15</td>
				    <td>600</td>
				</tr>

				<tr>
				    <td>s3_srvr_12</td>
				    <td>15</td>
				    <td>1500</td>
				</tr>

				<tr>
				    <td>s3_srvr_13</td>
				    <td>15</td>
				    <td>1500</td>
				</tr>

				<tr>
				    <td>osek_control</td>
				    <td>15</td>
				    <td>1500</td>
				</tr>
	
				<tr>
				    <td>space_control</td>
				    <td>15</td>
				    <td>1500</td>
				</tr>

				<tr>
				    <td>subway_control</td>
				    <td>15</td>
				    <td>1500</td>
				</tr>
				
			</table>

		<h2>Summary of Results</h2>
		        
		<ul>
            <li><b>RQ1:</b> In the data-flow testing w.r.t. all def-use coverage, what is the performance difference
between different search strategies, i.e., DFS, RSS, RSS-COS:md2u, SDGS and CPGS (our cut point guided path search strategy) in terms of testing time and number of covered pairs for the SE-based approach?</li>

			<p>In summary, our cut-point guided search (CPGS) strategy performs the best for data-flow testing. It improves 12∼40% data-flow coverage, and at the same time reduces the total testing time by 15∼48% and the number of explored paths by 28∼74%. Therefore, the SE-based approach, enhanced with the cut point guided search strategy, is effective for data-flow testing.</p>

            <li><b>RQ2:</b> How is the practicability of the SMC-based reduction approach (with the CEGAR-based and
BMC-based techniques) in terms of testing time and number of identified feasible and infeasible pairs?</li>

			<p>In summary, the SMC-based reduction approach in general is practical for data-flow testing. Both the CEGAR-based and BMC-based approaches can give consistent conclusions on the majority of def-use pairs. Specifically, the CEGAR-based approach can give answers for feasibility as certain, while the BMC-based approach can serve as a heuristic-criterion to identify hard-to-cover (probably infeasible) pairs if given appropriate checking bounds. In general, the CEGAR-based approach is more efficient on large and complicated subjects than the BMC-based approach in the context of data-flow testing.</p>

			<li><b>RQ3:</b> How efficient is the hybrid approach, which complements the SE-based approach with the
SMC-based approach, in terms of testing time and coverage level, compared with the SE-based approach or the SMC-based approach alone?</li>

			<p>In summary, the hybrid approach, which combines symbolic execution and software model checking, achieves more efficient data-flow testing. The model checking approach can weed out infeasible pairs that the symbolic execution approach cannot infer by 71.9%∼97.2%. Compared with KLEE alone, the hybrid approach can improve data-flow coverage by 27.8∼45.2%. In particular, KLEE+CPAchecker performs best, which reduces total testing time by 93.6% for
all 33 subjects, and at the same time improves data-flow coverage by 36.5%. Compared with the SMC-based approach alone, the hybrid approach can also reduce testing time by 19.9∼23.8%.</p>
        </ul>
   
	<p>For more information, please feel free to contact us: <a href="http://tingsu.github.io/">Ting Su</a>, Chengyu Zhang, Yichen Yan.</p>

	<h2>Reference</h2>

	<ul>
			<!--<li>Towards Efficient Data-flow Test Data Generation (under the review of a software engineering journal)</li>
	
            <li>Towards Efficient Data Flow Test Input Generation Using KLEE (KLEE Workshop, 2018)</li> -->

            <li>SmartUnit: Empirical Evaluations for Automated Unit Testing of Embedded Software in Industry (ICSE 2018 SEIP)</li>

			<li>A Survey on Data-Flow Testing (ACM Computing Surveys, 2017)</li>

			<li>Automated Coverage-driven Testing: Combining Symbolic Execution and Model Checking (SCIENCE CHINA Information Sciences, 2016)</li>

			<li>Combining Symbolic Execution and Model Checking for Data Flow Testing (ICSE 2015)</li>

			<li>Automated Coverage-Driven Test Data Generation Using Dynamic Symbolic Execution (SERE 2014, now renamed as QRS)</li>

    </ul>
    
    <p>Last modified: 2019.3</p>

    </body>
</html>
