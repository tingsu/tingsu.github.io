\documentclass[a4paper]{article}

\usepackage[hidelinks]{hyperref}

\title{RESEARCH PLAN}
\author{Ting Su}
\date{\today}

\setlength{\topmargin}{-10mm}
\setlength{\textwidth}{7in}
\setlength{\oddsidemargin}{-8mm}
\setlength{\textheight}{9in}
\setlength{\footskip}{1in}

\begin{document}
\fontsize{12}{15}
\selectfont
\maketitle

\section{Introduction}

Nvidia CEO Jensen Huang said ``Software is eating the world''. In fact, software is running everywhere (e.g., automobiles, aircraft, satellites, public equipment, personal devices), but at the same time,
the complexity and diversity of software have grown rapidly.
For example, the \emph{control software} running in a car has increased from 10 thousand lines of code in 1980 to 100 million lines of code in 2020.
Android, the most popular \emph{mobile software}, since its first release by Google in 2007, now it has over 10 major versions and is running on 24,000 distinct  device models. 
\emph{Deep learning software} recently is gaining more popularity in self-driving cars and robotic systems.
However, this situation brings many challenges on how to \emph{thoroughly} and \emph{effectively} ensure the quality, reliability and security of these different software systems, which typically running in complex environment.

To this end, my research aims to develop novel program analysis, testing and verification techniques to improve software reliability, security, and development productivity. In my research, I devote myself to convert these techniques into practical tools (not just prototypes), and benefit both academic researchers and industrial practitioners. 
In the short term (3 years), I plan to concentrate on the following three research directions: 
(1) quality assurance for control software systems, (2) correctness validation for machine learning software systems, and (3) automated testing for human-computer interaction software.

\section{Research Plan}

\subsection{Quality Assurance for Control Software Systems}

The reliability and security of control software in many IoT systems are very crucial in industry.
These software has several unique characteristics. 
First, they often use private or public protocols to realize data transmission, such as railway signal system, vehicle control system and satellite gesture control system. 
Second, they are usually layered-designed to achieve various functions via different API interfaces.
For example, the emerging industrial Internet platform has multiple layers, including device layer, cloud service layer, micro-service layer and application layer. 
However, existing software testing techniques cannot enforce thorough and effective protocol and interface validation. Many software errors are discovered post-deployment, and lead to severe losses. 

To this end, I plan to improve the reliability of control software in
IoT systems. I plan to focus on solving two key problems: 
(1) \emph{Effective protocol testing techniques}. 
For a specific software protocol, my idea is to extract protocol specifications as a state machine model, and design model-based testing techniques to do stress testing. 
This idea enables us to improve the testing efficiency by pruning protocol state space,
which is the main challenge in protocol testing.
(2) \emph{Effective interface testing techniques}.
For a specific platform architecture, I plan to use natural language processing technique to extract functional specification from its API documentation. 
Given an interface, the technique aims to generate non-homogenous API call sequences and parameters to stress test the functional correctness.
These techniques will lead to effective analysis and testing tools for IoT systems. Specifically, I plan to focus on smart home device and robotic software.

\subsection{Correctness Validating for Machine Learning Software Systems}

Recently, many artificial intelligence software systems achieved rapid development. Machine learning (especially deep learning) based software systems have been widely used in safety-critical areas such as autonomous driving, medical diagnostics, and industrial robotics. However, how to guarantee their reliability and security is still in their infancy. 

Taking autonomous driving as an example, this type of system mainly uses the deep neural network to automatically identify objects in the road image, combines sensor data, and controls the wheel steering and speed. However, according to incomplete statistics,
autonomous vehicles from Tesla and Uber have caused more than 10 serious casualties; 
according to a study in 2018, autonomous vehicles still require manual interventions, 64\% of which are caused by machine learning software defects.
However, unlike traditional software, machine learning software is driven by data, and software behavior is determined by the probability of the model generated by the training data rather than
a deterministic algorithm. This makes many traditional software testing and verification techniques lose effect.

To this end, I plan to investigate how to test and validate the correctness of machine learning software systems. 
My agenda includes two key research problems: (1) \emph{How to generate test data that is diverse and consistent with real-life scenarios}.
In autonomous driving, the current mainstream testing technique is to generate adversarial examples as test data. These test data is generated by changing the image pixels. Although these data can effectively disturb the judgment of the deep learning system, but is difficult to be understood and can hardly be used for error localization and debugging. 
Thus, I plan to use object recognition technology to effectively generate real and diverse test data by changing, adding, and deleting real objects in the image to improve the testing effectiveness.
(2) \emph{How to generate oracle-preserving test data}. Unlike traditional software, machine learning systems do not have clear functional specifications, e.g., what outputs should be expected given a particular input (this is called \emph{test oracle problem} in traditional software testing), which poses a great challenge for judging machine learning software errors. I plan to use the idea of \emph{metamorphic testing}, that is, given a test input, generating a mutant input that guarantees the output, so as to achieve high-precision, low false-positive testing. 
This technique can also be applied to test machine translation software systems, e.g., Google Translate.

%My recent work on designing testing adequacy criteria for deep neural network has received an ACM SIGSOFT Distinguished Paper Award in ASE 2018. Additionally, I have participated in several related research projects in this direction. Thus, I believe I have the ability to conduct more in-depth research in this area.

\subsection{Automated Testing for Human-Computer Interaction Software}

Nowadays, most software services are provided via human-computer interactions. Among them, graphical user interface (GUI)-based software is particularly popular, such as mobile software, in-vehicle software, smart home apps, and even industrial Internet apps. Take mobile software as an example, they provide a variety of services, such as e-banking services, third-party payments, map navigation, remote control and so on. The errors of these software may not directly compromise personal safety, but they can directly affect user loyalty and vendor reputation. However, unlike traditional software, these software targets ordinary users rather than professional developers. Thus, many challenges exists for effectively testing them. First, this type of software is highly application-specific and heavily depends on specific interaction scenarios, and traditional testing techniques are difficult to thoroughly cover these scenarios. For example, online shopping software requires specific shopping procedure to reach the payment page; and gaming software requires specific strategies to enter specific game scenario. 
Second, due to the features of human interactions and frequent GUI updates, test data design, maintenance and error detection heavily depends on human knowledge.
Third, these software can be easily affected by external environment, such as equipment status, geographic location, network, and so on.

To this end, I plan to conduct in-depth research for testing human-computer interaction software, especially for GUI-based software. My research plan includes: 
(1) \emph{Human knowledge guided test data generation}.
I plan to leverage existing developer test data, history user data, and AI techniques to produce application-specific test data. 
These test data can effectively guide testing to reach specific usage scenarios, and combine more systematic testing techniques to find deep software bugs.
(2) \emph{Property-based functional fuzzing and validation}.
GUI-based software (e.g., mobile software) usually does not maintain clear functional specifications or documentations. 
This makes it very difficult to verify the functional correctness.
Thus, I plan to leverage some invariant properties (e.g., interacting with independent GUI elements should not affect other GUI elements on the screen), and insert additional interaction events (i.e., the interactions on these independent GUI elements) into existing tests to generate new tests. These new interaction events can be viewed as various adverse conditions for validating functional correctness.

\section{Team Building and Research Outputs}
To achieve the above goals, I plan to build a research team with 2-3 post-docs, 3-4 PhD students and 4-6 master students. Each research topic will have 1 post-doc, 1 PhD and some master students. To maintain this research team, besides the startup fund, I will try to find funds from government and companies, and actively search for technology transfer.
The research results will be published in top-tier conferences or journals, e.g., ICSE, FSE, ASE, ISSTA, PLDI, OOPSLA, TSE and TOSEM.
I also plan to host international conferences or domestic workshops to improve research impact.

\section{Research Achievements and Impacts}

I give a short summary of my accomplished research achievements and impacts, which serves the basis of the above research plan.

{\noindent\textbf{Publications}: \#Total Papers: \textbf{29}, \#Top-tier Papers(CCF-A/SCI-T1) : \textbf{16}, Google Scholar Citation: 391
	
\noindent\textbf{Awards}: \textbf{3} ACM SIGSOFT Distinguished Paper Awards (ICSE 2018, ASE 2018, ASE 2019, all are CCF-A conferences);
 \textbf{2} Best Research Tool Awards (NASAC 2017, NASAC 2018), and \textbf{1} Golden Medal of ACM Student Research Competition (ICSE 2016, CCF-A), Nomination for Distinguished PhD Thesis (CCF, 2017)
 
\noindent\textbf{Techniques/Tools}: As the \emph{first author}, I developed these two tools:

 (1) \textbf{Stoat} (\url{https://github.com/tingsu/stoat}): the \emph{state-of-the-art} automated functional GUI fuzzing tool for mobile apps in the world. It has found \textbf{8000} fatal software errors from \textbf{9000} mobile apps, including \textbf{WeChat}, \textbf{Gmail} and \textbf{Google+}. Prof. Tao Xie (IEEE Fellow), Prof. Andreas Zeller (ACM Fellow), Prof. Jian Lv (Academician of Chinese Academy of Sciences) gave very positive remarks on my work. This work secured \emph{NTUitive GAP Fund with S\$ 240,250} for commercialization. Stoat is now widely used by world-renowned universities, including UIUC, NUS, UC Davis/Irvine, ETH Zurich, Peking University, Nanjing University.
 This line of work lead to several top-tier conference papers~\cite{fsmdroid,stoat,study,study2,apechecker,ausera,recdroid,ui2code,storydroid}.
 
 (2) \textbf{SmartRocket Unit} (\url{https://www.ticpsh.com/air.html}): the \emph{world-leading and first} automated intelligent testing tool for embedded industrial software in China. My Ph.D research work~\cite{sere14,icse15,csur17,scis16,smartunit} was successfully converted into this commercial product. It is now serving more than \textbf{10} domestic industrial companies (including China Academy of Space Technology, CASCO Signal Ltd, Guangzhou Automobile Group), tested more than \textbf{3 millions} lines of code, and recently received the German PUV Rheinland security certificate.
 
 (3) My research work also lead to other fuzzing and testing tools for different software systems, e.g., fuzzing Java Virtual Machines~\cite{classfuzz,classming}, validating formal verification tools~\cite{mcfuzz}, testing online gaming apps~\cite{wuji}, and testing deep learning systems~\cite{deepguage}.
  
\noindent\textbf{Media Reports}:

 1. Stoat: Golden Medal of ACM Student Research Competition
 
 \url{http://www.sei.ecnu.edu.cn/Data/View/2175} (by ECNU)
 
 2. Stoat: Best Research Tool Award, 
 
 \url{http://www.sohu.com/a/207736248_610499} (by Software Engineering Research and Practice)
 
 3. Large-scale fault study for Android apps
 
 \url{https://mp.weixin.qq.com/s/zinnpUWWRVmguaoUoETCvg} (by CodeWisdom)
 
 4. Automatic GUI Implementation
 
 \url{https://www.jiqizhixin.com/articles/2018-12-02-2} (by jiqizhixin)

%During my Ph.D study (2011-2016), I focus on the problem of how to automatically and efficiently generate test data (inputs) to improve software testing, especially for safety-critical embedded software. \emph{Generating valid and effective test data is very crucial to finding software bugs, which however in practice occupies more than 50\% software development costs.}
%To this end, my research goal is to reveal software bugs as early as possible and reduce human efforts as much as possible. 
%Specifically, I investigated the testing requirements in the real-world context from ten selected research partner companies in China, covering many safety-critical fields like railway, aerospace, nuclear plant, and automobile.
%We find unit testing is a mandatory task required in various international standards, e.g., IEC 61508, ISO26262, RTCA DO-178B/C, etc for testing industrial systems. Moreover, different test adequacy criteria are demanded to satisfy different software safety and integrity levels.
%
%To achieve this goal, I adapted \emph{dynamic symbolic execution} technique and proposed several efficient test data generation algorithms and strategies~\cite{sere14,icse15,csur17,scis16,smartunit}.
%I spent three years in developing and maintaining a tool named CAUT, a symbolic execution based automatic testing tool for \texttt{C} program. CAUT can automatically generate test data for both control-flow criteria (e.g., statement, branch and MC/DC coverage)~\cite{sere14} and data-flow criteria (e.g., all def-use coverage)~\cite{icse15,csur17}.
%Further, I systematically adapt software model checking technique to complement symbolic execution. This combined approach can effectively eliminate infeasible test objectives (i.e., no test data exists for covering such test objectives), and thus further reduce human inspection efforts. Now, CAUT has been commercialized into the first industrial unit testing tool in China, named SmartRocket Unit~\cite{smartunit}, and is serving more than ten large domestic companies, including China Academy of Space Technology, CASCO Signal Ltd, Guangzhou Automobile Group.
%
%Since 2015, I start to looking into the problem of how to effectively test GUI-based software, especially for mobile apps. This line of work also lasts during my whole postdoctoral research (2016-now). \emph{GUI-based software is very popular event-driven software, which is notoriously difficult to test and heavily relies on expensive, painstaking manual testing.}
%Unlike traditional embedded software, GUI software heavily interacts with complex environments (e.g., humans, device sensors and other apps), which poses several unique challenges for validating their correctness.
%For example, mobile apps (e.g., Android, iOS) contains diverse GUI widgets, runs on fast-evolving (and notoriously fragmented) mobile platforms, and requires very short delivery cycle. To this end, my research goal is to develop effective analysis, testing, security techniques to ensure their reliability and security.
%
%To put this goal into practice, I spent two years and developed a state-of-the-art GUI testing tool \emph{\textbf{Stoat}} (\url{https://github.com/tingsu/stoat}) for Android apps~\cite{stoat}. Stoat has extensively tested 9,000+ open-source and commercial apps and found 8,000+ fatal crashes in one year. 
%Based on these crash bugs and online data from open-source hosting platforms (e.g., \texttt{Github}, \texttt{Google Code}), I lead the work of conducting the first large-scale and most comprehensive fault study of Android apps~\cite{study,study2}. We investigate these bugs from several perspectives: (1)
%their characteristics (e.g., manifestation locations, fault taxonomies), (2) the developers’ testing practice, (3) the abilities of existing bug
%detection techniques, (4) their reproducibility and (5) bug fixes. This study motivates several follow-up research directions.
%Inspired by this study, I further lead the work of detecting asynchronous programming errors (APEs) in Android apps~\cite{apechecker}. The key idea is to characterize APEs as specific fault patterns, and synergistically combine static analysis and dynamic UI exploration to detect and verify such errors.
%We developed a tool APEChecker to enforce this idea, which greatly improves the-state-of-art and reduces testing time from half an hour to a few minutes for each app. 
%To improve the security of Android apps, I also work on leveraging static taint analysis technique to identify potential security weaknesses in mobile banking apps~\cite{ausera}, which may leak sensitive user information (e.g., identities, passwords).
%
%Additionally, I work on automated bug reproduction, GUI code generation, GUI design comprehension for Android apps~\cite{recdroid,ui2code,storydroid}.
%We leverage natural language processing (NLP) techniques to automatically reproduce app crashes from human-written bug reports~\cite{recdroid}.
%We use deep learning techniques to automatically generate GUI skeleton code to boost mobile GUI implementation~\cite{ui2code}.
%We use reinforcement learning techniques to help identify potential defects in large mobile game apps~\cite{wuji}.
%These proposed solutions significantly improve development efficiency and productivity.
%
%During my postdoctoral research (2016-now), I am also particularly interested in validating the correctness of fundamental software that we depend on. These fundamental software includes compilers and program analyzers. For example, compiler bugs can lead to bugs in other programs, weaken source-code level analysis and hard to notice. Program analyzer bugs may lead to incorrect analysis or verification results of the programs under checking, which could miss critical software defects. In this direction, I work on using differential testing techniques to stress-test Java Virtual Machines (JVMs)~\cite{classfuzz,classming} and reachability fuzzing to validate the correctness software model checkers~\cite{mcfuzz}. Now, I am also looking into how to effectively test, secure and understand deep learning systems~\cite{deepguage}, whose correctness is also very important in some safety-critical domains, e.g., self-driving and medical diagnosis.
%
%
%\section{Research Achievements and Impacts}
%
%Section~\ref{highlight} shows my research achievements and impacts, and Section~\ref{mobile},~\ref{unit_testing},~\ref{testing_analyzers} give more details.
%
%\subsection{Highlight}
%\label{highlight}
%
%My research work is mainly published on top-tier conferences and journals in the fields of software engineering and programming languages. In the recent five years, I have published \underline{29} papers. Among them, \underline{16} papers are top-tier papers (\underline{11} are published as the first author or corresponding author), and ICSE (7 full papers), FSE (3 full papers), ASE (3 full papers), PLDI (1 full paper) and CSUR (1 full paper). To date, my Google Scholar citation is \underline{387}, and H-index is \underline{10}.
%
%In the field of software testing and analysis, I, as the first author, won \emph{First Place (Golden Medal) of ACM Student Research Competition} in ICSE 2016 and \emph{First Place of Research Tool Competition} in NASAC 2017 (the flagship software engineering and system conference in China); as the co-first author and corresponding author, I won an \emph{ACM SIGSOFT Distinguished Paper Award} in ICSE 2018 for conducting the first fault study for mobile software. I won another \emph{ACM SIGSOFT Distinguished Paper Award} in ASE 2018 for testing deep learning systems, and a third \emph{ACM SIGSOFT Distinguished Paper Award} in ASE 2019 for using deep learning techniques to test mobile game apps. As the first author, I proposed and developed \emph{Stoat}, a stochastic, model-based GUI testing technique, which is now one of state-of-art GUI testing tool, and widely used by many researchers and industrial practitioners.
%As the first author, I also proposed and developed the core techniques of \emph{SmartRocket Unit}, which is already converted into a commercial unit testing tool for industrial embedded software. It is now severing more than 10 Chinese domestic leading companies. \emph{SmartRocket Unit} has accumulatively tested millions lines of code, and made significant contributions to industrial software reliability.
%
%
%\subsection{Mobile Software Analysis, Testing and Security}
%\label{mobile}
%
%I lead several research work on analyzing, testing, debugging, and securing mobile applications. The \emph{key goal} is to sharpen the competitive edge of mobile apps, and ensure their reliability and security.
%All these work has been published in top-tier software engineering conferences, including ICSE (5 papers), FSE (2 papers), ASE (2 papers).
%
%As the first author, I spent two years and developed a state-of-the-art GUI testing tool \emph{\textbf{Stoat}} (\url{https://github.com/tingsu/stoat}) for Android apps.
%Stoat enforces a guided, stochastic model-based GUI testing technique.
%To date, Stoat has been deployed to extensively test \underline{9,000+} open-source and commercial apps and found \underline{8,000+} fatal crashes. Stoat has helped find several bugs in such popular apps as \texttt{WeChat}, \texttt{Google+}, \texttt{Gmail} that have billions of users. All these bugs have been confirmed and fixed. 
%
%Stoat won several awards: the \emph{First Place of ACM Student Research Competition} in ICSE 2016, \emph{First Place of Research Tool Competition} in NASAC 2017 (the flagship software engineering and system conference in China), and received \emph{NTUitive Gap Fund Grant} S\$240,250 in 2018 for commercialization potential. Stoat was recognized as ``\emph{a state-of-the-art tool, (which) is able to trigger the highest number of unique crashes (in Android apps)}'' by a recent ASE 2018 paper from the research group of IEEE Fellow Prof. Tao Xie; ``\emph{Stoat is effective for discovering unique crashes}'' by a recent ISSTA 2019 paper from the research group of ACM Fellow Prof. Andreas Zeller; ``\emph{We compare our tool with three state-of-art testing tools, i.e., Monkey, Sapienz and Stoat}'' mentioned by a recent ICSE 2019 paper from the research group of ETH Zurich.
%Stoat has also been widely used by the researchers from many renowned institutes worldwide, e.g., UIUC, UC Irvine, ETH Zurich, Singapore National University, Peking University, Nanjing University. See the demo of Stoat testing platform: \url{https://youtu.be/41jzFM7WhP4}
%
%On the basis of Stoat, as the co-first author (also the corresponding author), I lead the largest and most comprehensive fault study for Android apps (\url{https://crashanalysis.github.io/}), based on the bugs detected by Stoat and the data from open-source hosting platforms (e.g., \texttt{Github}, \texttt{Google Code}). This study involves \underline{250,000} crash bugs from \underline{2,486} open-source apps and \underline{3,230} commercial apps, and insightful feedback from \underline{135} professional app developers worldwide. We investigate the key question ``\emph{why an Android app may crash?}'', understand how app developers analyze, test, reproduce and fix app crashes, and motivate several follow-up research directions. This work won an \emph{ACM SIGSOFT Distinguished Paper Award} in ICSE 2018. Our dataset and benchmark have been requested by more than 13 institutes worldwide (e.g., CMU, KAIST, UNSW, Monash, HKUST). Due to these work, I was invited to give talks in A-Mobile 2018 (a ASE 2018 workshop) and ISEC 2019 (the flagship software engineering conference in India)
%Following this study, we designed more effective testing techniques (Stoat+ and APEChecker) for detecting intricate bug types (e.g., asynchronous programming errors). 
%
%We also conducted a large-scale automated security risk assessment for mobile banking apps. This study revealed \underline{2,157} security weaknesses in \underline{693} banking apps worldwide. To date, \underline{21} banking entities have confirmed \underline{126} weaknesses, and \underline{52} weaknesses have been patched. This work has been well-acknowledged by some bank entities from HonKong, Singapore and India, e.g., \texttt{HSBC}, \texttt{OCBC}, \texttt{DBS} and \texttt{BHIM}. The tool \textbf{Ausera} won \emph{Third Place of Research Tool Competition} in NASAC 2018.%, and now has been integrated into one security scanning product of \href{https://scantist.com/}{\texttt{Scantist}}. 
%
%
%\subsection{Automated Testing for Industrial Embedded Software}
%\label{unit_testing}
%
%During the first three years of my Ph.D, I invested tremendous efforts in the development and maintenance of CAUT (\url{https://github.com/tingsu/caut-lib}), a symbolic execution based automatic testing tool for \texttt{C} program. CAUT's main novelty is to automatically, efficiently generate test data to satisfy different types of code adequacy criteria, including both control-flow and data-flow. The related papers were published in SERE 2014, ICSE 2015, CSUR 2017, and ICSE 2018.
%
%CAUT has been successfully applied to test several industrial embedded software, including an automobile engine management system, a satellite gesture control system, and a subway signal control system.  According to the feedback from our partner companies, on average, CAUT reduced 60\% human efforts, improved 20\% code coverage, and revealed several critical bugs in these products. 
%
%Later, as a co-founder, I significantly contributed to \textbf{SmartRocket Unit} (\url{https://www.ticpsh.com/air.html}), a commercial automated unit testing tool inherited from CAUT. SmartRocket Unit fully supports unit testing for statement, branch, boundary value and MC/DC coverage, and has been applied in several industry-scale embedded control systems, and accumulatively tested \underline{3 millions lines} of C code, and improved testing efficiency by \underline{ten times}.
%To date, it is serving more than ten Chinese domestic companies, including \texttt{CASCO} (the best railway signal
%corporation in China), \texttt{GAC MOTOR} (one of biggest car manufacturers in China), \texttt{CAST} (the main spacecraft production agency in China). It recently received the German PUV Rheinland security certificate.
%
%
%\subsection{Testing Compilers and Program Analyzers}
%\label{testing_analyzers}
%
%I also worked on testing compilers (e.g., Java Virtual Machines) and program analyzers (e.g., software model checkers, SAT/SMT solvers). The \emph{key goal} is to ensure the correctness and reliability of many fundamental development tools we depend on. These work was published in PLDI 2016, ICSE 2019 and FSE 2019.
%
%We proposed two stress-testing techniques \emph{\textbf{classming}} and \emph{\textbf{classfuzz}} for JVMs. We discovered \underline{62+} JVM discrepancies in the start-up process of \texttt{Oracle}'s HotSpot and \texttt{IBM}'s J9, which lead to  several clarifications and changes to the Java SE 8 edition of the JVM specification; We revealed \underline{30+} fatal bugs in \texttt{Oracle}'s HotSpot and \texttt{IBM}'s J9. In particular, we found one highly-critical security vulnerability (CVE-2017-1376, CVSS base score \underline{9.8}), which directly affects \underline{6} IBM's products and \underline{99} IBM's derivative products.
%
%To validate the correctness of software model checkers, we proposed an effective testing technique \textbf{\emph{\href{https://github.com/MCFuzzer/MCFuzz}{MCFuzz}}}. 
%This is the \emph{only available effective technique in the world} to tackle the challenging problem of how to validating software model checkers.
%MCFuzz has extensively tested three state-of-the-art and state-of-the-practice C software model checkers, \texttt{CPAchecker}, \texttt{CBMC}, and \texttt{SeaHorn}. We have found bugs in all
%three model checkers. To date, we have reported a total of \underline{62} unique bugs --- \underline{59} were previously unknown, \underline{58} have been confirmed, and \underline{20} have been fixed. We are well-appreciated by the model checker developers.
%Daniel Kroening, a professor from University of Oxford, who proposed and developed CBMC, thanked our bug reports in person and added our generated new tests into their test suites ``\emph{Many thanks, test added}''.
%The research group lead by Dirk Beyer, a professor from LMU Munich, who proposed and developed CPAchecker, also thanked us for our bug reports ``\emph{Thank you very much for finding and reporting these cases! This is very helpful for us}''.
%
%\section{Research Agenda}
%
%I plan to investigate the following research directions in the near future.







\begin{thebibliography}{99}
	
	\bibitem{sere14}
	\newblock Ting Su, Siyuan Jiang, Geguang Pu, Bin Fang, Jifeng He, Jun Yan and Jianjun Zhao.
	\newblock Automated Coverage-Driven Test Data Generation Using Dynamic Symbolic Execution.
	\newblock {\em Eighth International Conference on Software Security and Reliability}, SERE'14, 2014.
	
	\bibitem{icse15}
	\newblock Ting Su, Zhoulai Fu, Geguang Pu, Jifeng He, and Zhendong Su.
	\newblock Combining Symbolic Execution and Model Checking for Data Flow Testing.
	\newblock {\em 37th {IEEE/ACM} International Conference on Software Engineering}, ICSE'15, 2015.
	
	\bibitem{csur17}
	\newblock Ting Su, Ke Wu, Weikai Miao, Geguang Pu, Jifeng He, Yuting Chen and Zhendong Su.
	\newblock A Survey on Data-Flow Testing.
	\newblock {\em ACM Computing Surveys}, 2017.
	
	\bibitem{scis16}
	\newblock Ting Su, Geguang Pu, Weikai Miao, Jifeng He, and Zhendong Su.
	\newblock Automated Coverage-driven Testing: Combining Symbolic Execution and Model Checking. 
	\newblock {\em SCIENCE CHINA Information Sciences}, 2016.

	\bibitem{smartunit}
	\newblock Chengyu Zhang, Yichen Yan, Hanru Zhou, Yinbo Yao, Ke Wu, Ting Su, Weikai Miao and Geguang Pu.
	\newblock SmartUnit: Empirical Evaluations for Automated Unit Testing of Embedded Software in Industry.
	\newblock {\em 	The 40th International Conference on Software Engineering (SEIP)}, ICSE'18, 2018.
	
	\bibitem{fsmdroid}
	\newblock Ting Su.
	\newblock FSMdroid: Guided GUI Testing of Android Apps.
	\newblock {\em 	The 38th International Conference on Software Engineering}, ICSE'16, 2016.
	
	\bibitem{stoat}
	\newblock Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang Pu, Yang Liu, and Zhendong Su.
	\newblock Guided, Stochastic Model-Based GUI Testing of Android Apps.
	\newblock {\em 	The 11th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering}, FSE'17, 2017.
	
	\bibitem{study}
	\newblock Lingling Fan, Ting Su, Sen Chen, Guozhu Meng, Yang Liu, Lihua Xu, Geguang Pu and Zhendong Su.
	\newblock Large-Scale Analysis of Framework-Specific Exceptions in Android Apps.
	\newblock {\em 	The 40th International Conference on Software Engineering}, ICSE'18, 2018.
	
	\bibitem{study2}
	\newblock Ting Su, Lingling Fan, Sen Chen, Yang Liu, Lihua Xu, Geguang Pu and Zhendong Su.
	\newblock Why My App Crashes? Understanding and Benchmarking
	Framework-specific Exceptions of Android apps. 
	\newblock {\em 	IEEE Transactions on Software
		Engineering}, TSE 2020 (minor revision).
	
	\bibitem{apechecker}
	\newblock Lingling Fan, Ting Su, Sen Chen, Guozhu Meng, Yang Liu, Lihua Xu, and Geguang Pu.
	\newblock Efficiently Manifesting Asynchronous Programming Errors in Android Apps.
	\newblock {\em The 33rd IEEE/ACM International Conference on Automated Software Engineering}, ASE'18, 2018.
	
	\bibitem{ausera}
	\newblock Sen Chen, Ting Su, Lingling Fan, Guozhu Meng, Minhui Xue, Yang Liu, Lihua Xu.
	\newblock Are Mobile Banking Apps Secure? What Can be Improved?.
	\newblock {\em 	The 26th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering}, FSE'18, 2018.
	
	\bibitem{recdroid}
	\newblock Yu Zhao, Tingting Yu, Ting Su, Yang Liu, Wei Zheng, Jingzhi Zhang, and William G.J. Halfond.
	\newblock StoryDroid: Automated Generation of Storyboard for Android Apps.
	\newblock {\em 	The 41th International Conference on Software Engineering}, ICSE'19, 2019.
	
	\bibitem{ui2code}
	\newblock Chunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing and Yang Liu .
	\newblock From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation.
	\newblock {\em 	The 40th International Conference on Software Engineering}, ICSE'18, 2018.
	
	\bibitem{storydroid}
	\newblock Sen Chen, Lingling Fan, Chunyang Chen, Ting Su, Wenhe Li, Yang Liu, Lihua Xu.
	\newblock Large-Scale Analysis of Framework-Specific Exceptions in Android Apps.
	\newblock {\em 	The 41th International Conference on Software Engineering}, ICSE'19, 2019.
	
	\bibitem{classfuzz}
	\newblock Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao.  
	\newblock Coverage-Directed Differential Testing of JVM Implementations.
	\newblock {\em ACM SIGPLAN Conference on Programming Language Design and Implementation}, PLDI'16, 2016.
	
	\bibitem{classming}
	\newblock Yuting Chen, Ting Su, and Zhendong Su.
	\newblock Deep Differential Testing of JVM Implementations.
	\newblock {\em The 41st ACM/IEEE International Conference on Software Engineering}, ICSE'19, 2019
	
	\bibitem{mcfuzz}
	\newblock Chengyu Zhang, Ting Su, Yichen Yan, Fuyuan Zhang, Geguang Pu and Zhendong Su.  
	\newblock Finding and Understanding Bugs in Software Model Checkers.  
	\newblock {\em 27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, FSE'19, 2019. 
	
	\bibitem{wuji}
	\newblock Yan Zheng, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu, Ruimin Shen, Yinfeng Chen and Changjie Fan.
	\newblock Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning.
	\newblock {\em 34th IEEE/ACM International Conference on Automated Software Engineering}, ASE'19, 2019. 
	
	\bibitem{deepguage}
	\newblock Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, and Yadong Wang.
	\newblock DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems.
	\newblock {\em The 33rd IEEE/ACM International Conference on Automated Software Engineering}, ASE'18, 2018. 
	
\end{thebibliography}


\end{document}
